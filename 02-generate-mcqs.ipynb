{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78ee31c7",
      "metadata": {
        "id": "78ee31c7"
      },
      "source": [
        "# Generate MCQs using MedAlpaca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Gdxu0m95uIB3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdxu0m95uIB3",
        "outputId": "3bdb3785-54a4-4683-8c5e-975be3b5c364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # Upload JSONL file with question and answer fields\n",
        "# uploaded = files.upload()\n",
        "# file_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Get input from Google Drive\n",
        "file_path = Path(\"/content/drive/MyDrive/medquad_sampled.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "88Wx0bobs4na",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Wx0bobs4na",
        "outputId": "4d0819c6-88d4-4863-c5cc-6530fc31d2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# checkpoint = \"microsoft/phi-2\"\n",
        "# checkpoint = \"stanford-crfm/BioMedLM\"\n",
        "checkpoint = \"medalpaca/medalpaca-7b\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DzG25Uf-y_pf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "91189a8ae63a49a3a2b7a3ed929f19fd",
            "61ac4e36d28c49aaadb92835bd65b606",
            "625cc988243a409683f78c6865b91069",
            "515eb1f8ce0143879a5f6e60a7c9a7ee",
            "98b4d948810b4ecb828dc47f39e29563",
            "da4a5bb65c20485490a6bcb1e007059d",
            "9ea98ddaaced4595a91787ec7258ec89",
            "32a4d57ea21746298df2b97832d1ab6f",
            "93a9157feca74333a5f9744da5b4e9f8",
            "082cff0e69604cb59715bc3c663d1f0c",
            "1a97923595a7491293b6e60b04e04789"
          ]
        },
        "id": "DzG25Uf-y_pf",
        "outputId": "a8ecb342-dae0-4671-ba9e-54394df3ae9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3000 QA pairs from /content/drive/MyDrive/medquad_sampled.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:609: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91189a8ae63a49a3a2b7a3ed929f19fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer and model loaded.\n"
          ]
        }
      ],
      "source": [
        "# Load and parse JSONL file\n",
        "with open(file_path, \"r\") as f:\n",
        "    qa_data = [json.loads(line) for line in f]\n",
        "    print(f\"Loaded {len(qa_data)} QA pairs from {file_path}\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.float16).to(device)\n",
        "model.eval()\n",
        "print('Tokenizer and model loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1275ee88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1275ee88",
        "outputId": "4ff15c7c-c128-41bb-98b4-63778e025f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Done! 825 / 3000 (27.50%) generated MCQs saved to /content/drive/MyDrive/generated_mcqs.jsonl\n"
          ]
        }
      ],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# Check if output file already exists\n",
        "# Set output path in Google Drive\n",
        "drive_path = Path(\"/content/drive/MyDrive/generated_mcqs.jsonl\")\n",
        "# Check if output file already exists\n",
        "output_file = drive_path\n",
        "if output_file.exists():\n",
        "    response = input(f\"File {output_file} exists. Delete it? (y/n): \")\n",
        "    if response.lower().startswith(\"y\"):\n",
        "        output_file.unlink()\n",
        "    else:\n",
        "        raise RuntimeError(\"Aborting to avoid overwriting the file.\")\n",
        "\n",
        "# Few-shot example to encourage structure\n",
        "few_shot_example = \"\"\"\n",
        "Question: What is asthma?\n",
        "Answer: Asthma is a lung condition that causes airway inflammation and breathing difficulty.\n",
        "<choices>\n",
        "A. A skin condition\n",
        "B. A viral infection\n",
        "C. A lung disease that causes airway inflammation\n",
        "D. A type of heart failure\n",
        "</choices>\n",
        "<reason>Asthma is a lung condition that causes airway inflammation and breathing difficulty.</reason>\n",
        "<answer>C</answer>\n",
        "\"\"\"\n",
        "\n",
        "# Format QA into prompt\n",
        "def format_prompt(question, answer):\n",
        "    return f\"\"\"\n",
        "Create layperson multiple-choice answer choices and reasoning for the given medical Q&A pair.\n",
        "Include 4 total choices: one correct and three plausible, but incorrect. Follow the XML-style structure below:\n",
        "{few_shot_example}\n",
        "Now it's your turn:\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\"\"\"\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Batch-generate MCQs\n",
        "BATCH_SIZE = 16\n",
        "NUM_RETURN_SEQUENCES = 3\n",
        "num_prompts = 0\n",
        "num_success = 0\n",
        "# for i in tqdm(range(0, BATCH_SIZE*4+1, BATCH_SIZE),\n",
        "#               desc=f\"LLM Running on Micro Batches {BATCH_SIZE}\"):  # DEBUG\n",
        "with open(output_file, \"a\") as f:\n",
        "    for i in tqdm(range(0, len(qa_data), BATCH_SIZE),\n",
        "                        desc=f\"LLM Running on Micro Batches {BATCH_SIZE}\"):\n",
        "        batch = qa_data[i:i + BATCH_SIZE]\n",
        "        prompts = [format_prompt(q[\"question\"], q[\"answer\"]) for q in batch]\n",
        "\n",
        "        inputs = tokenizer(prompts, return_tensors=\"pt\",\n",
        "                          truncation=True,\n",
        "                          max_length=250,\n",
        "                          padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=250,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                num_return_sequences=NUM_RETURN_SEQUENCES,\n",
        "            )\n",
        "\n",
        "        prompt_lengths = [len(x) for x in inputs[\"input_ids\"]]\n",
        "\n",
        "        for j in range(len(batch)):\n",
        "            num_prompts += 1\n",
        "            for k in range(NUM_RETURN_SEQUENCES):\n",
        "                idx = j * NUM_RETURN_SEQUENCES + k\n",
        "                generated_tokens = outputs[idx][prompt_lengths[j]:]  # Strip prompt\n",
        "                completion = tokenizer.decode(generated_tokens, clean_up_tokenization_spaces=False)\n",
        "\n",
        "                mcq_choices = re.search(r\"<choices>(.*?)</choices>\", completion, re.DOTALL)\n",
        "                mcq_reason = re.search(r\"<reason>(.*?)</reason>\", completion, re.DOTALL)\n",
        "                mcq_answer = re.search(r\"<answer>(.*?)</answer>\", completion, re.DOTALL)\n",
        "                four_choices = 'A. ' in completion and 'B. ' in completion and 'C. ' in completion and 'D. ' in completion and 'E. ' not in completion\n",
        "\n",
        "                if not all([mcq_choices, mcq_reason, mcq_answer, four_choices]):\n",
        "                    continue\n",
        "                else:\n",
        "                    # Add to output json\n",
        "                    entry = {\n",
        "                          \"question\": batch[j][\"question\"],\n",
        "                          \"mcq_choices\": mcq_choices.group(1).strip(),\n",
        "                          \"mcq_reason\": mcq_reason.group(1).strip(),\n",
        "                          \"mcq_answer\": mcq_answer.group(1).strip(),\n",
        "                    }\n",
        "                    f.write(json.dumps(entry) + \"\\n\")\n",
        "                    f.flush()  # Save immediately\n",
        "                    num_success += 1\n",
        "                    break\n",
        "        clear_output(wait=True)\n",
        "        print(f\"\\n{num_success} / {num_prompts} ({num_success/num_prompts * 100:.2f}%) generated MCQs saved to {output_file}\")\n",
        "clear_output(wait=True)\n",
        "print(f\"\\nDone! {num_success} / {num_prompts} ({num_success/num_prompts * 100:.2f}%) generated MCQs saved to {output_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "082cff0e69604cb59715bc3c663d1f0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a97923595a7491293b6e60b04e04789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32a4d57ea21746298df2b97832d1ab6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "515eb1f8ce0143879a5f6e60a7c9a7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_082cff0e69604cb59715bc3c663d1f0c",
            "placeholder": "​",
            "style": "IPY_MODEL_1a97923595a7491293b6e60b04e04789",
            "value": " 3/3 [00:03&lt;00:00,  1.01s/it]"
          }
        },
        "61ac4e36d28c49aaadb92835bd65b606": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da4a5bb65c20485490a6bcb1e007059d",
            "placeholder": "​",
            "style": "IPY_MODEL_9ea98ddaaced4595a91787ec7258ec89",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "625cc988243a409683f78c6865b91069": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a4d57ea21746298df2b97832d1ab6f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93a9157feca74333a5f9744da5b4e9f8",
            "value": 3
          }
        },
        "91189a8ae63a49a3a2b7a3ed929f19fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61ac4e36d28c49aaadb92835bd65b606",
              "IPY_MODEL_625cc988243a409683f78c6865b91069",
              "IPY_MODEL_515eb1f8ce0143879a5f6e60a7c9a7ee"
            ],
            "layout": "IPY_MODEL_98b4d948810b4ecb828dc47f39e29563"
          }
        },
        "93a9157feca74333a5f9744da5b4e9f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98b4d948810b4ecb828dc47f39e29563": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea98ddaaced4595a91787ec7258ec89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da4a5bb65c20485490a6bcb1e007059d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
